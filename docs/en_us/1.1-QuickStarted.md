# Getting Started

## Development Approach

MaaFramework supports complete low-code programming through JSON (Pipeline JSON) and also provides [APIs](2.1-Integration.md) for developers to integrate on their own.  
It is also possible to combine both approaches, using low-code as a form of "wrapper" for invocation.  
Below are several common integration methods:

### Complete Dependence on JSON Low-Code Programming (Using MaaPiCli.exe)

This method is simple and quick, but not very flexible. It is recommended for beginners and those new to programming with MaaFramework.  
We provide a [üéûÔ∏è video tutorial](https://www.bilibili.com/video/BV1yr421E7MW) and [‚≠ê project template](https://github.com/MaaXYZ/MaaPracticeBoilerplate) for this method. Here is an example:

```jsonc
// JSON does not support comments; this is pseudo-code for reference only and cannot be executed directly
{
    "Recognize and Click Start Button": {
        "recognition": "OCR",           // Optical Character Recognition
        "expected": "Start",           // Text to recognize
        "action": "Click",             // Action: Click
        "next": [
            "Recognize and Click Confirm Icon"
        ]
    },
    "Recognize and Click Confirm Icon": {
        "recognition": "TemplateMatch", // Image template matching
        "template": "Confirm.png",       // Image file name
        "action": "Click"
    }
}
```

### Use JSON Low-Code Programming with Custom Logic for Complex Tasks

Launch the CLI through the API while registering some custom tasks. This method allows a seamless switch from method 1. Here is an example:

```jsonc
{
    "Recognize and Click Confirm Icon": {
        "next": [
            "My Custom Task"
        ]
    },
    "My Custom Task": {
        "recognition": "Custom",
        "custom_recognition": "MyReco",
        "action": "Custom",
        "custom_action": "MyAct"
    }
}
```

```python
# This is pseudo-code for reference only and cannot be executed directly
def main():
    # Register custom recognition
    maafw.Toolkit.register_custom_recognition("MyReco", MyRecognition())

    # Register custom action
    maafw.Toolkit.register_custom_action("MyAct", MyAction())

    # Start MaaPiCli
    maafw.Toolkit.run_pi_cli("C:/MaaXXX/resource", "C:/MaaXXX/cache")

class MyRecognition(CustomRecognition):
    def analyze(context, ...):
        # Obtain the image and perform custom image processing
        image = context.tasker.controller.cached_image
        # Return image analysis result
        return AnalyzeResult(box=(10, 10, 100, 100))

class MyAction(CustomAction):
    def run(context, ...):
        # Perform click action
        context.controller.post_click(100, 10).wait()
        # Override the next tasks to execute
        context.override_next(task_name, ["TaskA", "TaskB"])
```

### Write Your Own Code

You can use low-code as a "wrapper" for invocation or register custom callbacks.

```python
# This is pseudo code, for reference only, and cannot be run directly
# "Recognize and click the start button", "Recognize and click the confirmation icon" and so on are all logic in Json
def main():
    detail = tasker.post_pipeline("Recognize and click the start button").wait().get()

    if detail.completed:
        tasker.controller.post_click(100, 100).wait()

    else:
        image = tasker.controller.cached_image
        save_to_file(image)

        tasker.resource.register_custom_action("MyAction", MyAction())
        tasker.post_pipeline("Recognize and click the confirmation icon").wait()

    image: np.ndarray = tasker.controller.post_screencap().wait().get()
```

## Prepare resource files

*‚≠êIf you use the project template, just modify it directly in [folder](https://github.com/MaaXYZ/MaaPracticeBoilerplate/tree/main/assets/resource).*

You need to prepare some resource files with the typical file structure as follows:

```tree
my_resource
‚îú‚îÄ‚îÄ image
‚îÇ   ‚îú‚îÄ‚îÄ my_image_1.png
‚îÇ   ‚îî‚îÄ‚îÄ my_image_2.png
‚îú‚îÄ‚îÄ model
‚îÇ   ‚îî‚îÄ‚îÄ ocr
‚îÇ       ‚îú‚îÄ‚îÄ det.onnx
‚îÇ       ‚îú‚îÄ‚îÄ keys.txt
‚îÇ       ‚îî‚îÄ‚îÄ rec.onnx
‚îî‚îÄ‚îÄ pipeline
    ‚îú‚îÄ‚îÄ my_pipeline_1.json
    ‚îî‚îÄ‚îÄ my_pipeline_2.json
```

You can modify the names of files and folders starting with "my_", but the others have fixed file names and should not be changed. Here's a breakdown:

### Pipeline JSON Files

The files in `my_resource/pipeline` contain the main script execution logic and recursively read all JSON format files in the directory.

You can refer to the [Task Pipeline Protocol](3.1-PipelineProtocol.md) for writing these files. You can find a simple [demo](https://github.com/MaaXYZ/MaaFramework/blob/main/sample/resource/pipeline/sample.json) for reference.

Tools:

- [JSON Schema](https://github.com/MaaXYZ/MaaFramework/blob/main/tools/pipeline.schema.json)
- [VSCode Extension](https://marketplace.visualstudio.com/items?itemName=nekosu.maa-support)
  - Config resources based on `interface.json`
  - Support going to task definition, finding task references, renaming task, completing task, click to launch task
  - Support launching as MaaPiCli
  - Support screencap and crop image after connected

### Image Files

The files in `my_resource/image` are primarily used for template matching images, feature detection images, and other images required by the pipeline. They are read based on the `template` and other fields specified in the pipeline.

Please note that the images used need to be cropped from the lossless original image and scaled to 720p. If you use an Android emulator, please use the screenshot function that comes with the emulator! (You cannot directly take screenshots of the emulator window)

**UNLESS YOU EXACTLY KNOW HOW MAAFRAMEWORK PROCESSES, DO USE THE CROPPING TOOLS BELOW TO OBTAIN IMAGES.**

- [Image Cropping and ROI Extraction Tool](https://github.com/MaaXYZ/MaaFramework/tree/main/tools/ImageCropper)
- [VSCode Extension](https://marketplace.visualstudio.com/items?itemName=nekosu.maa-support)
- [MFA Tools](https://github.com/SweetSmellFox/MFATools)

### Text Recognition Model Files

*‚≠êIf you use the project template, just follow its documentation and run `configure.py` to automatically deploy the model file.*

The files in `my_resource/model/ocr` are ONNX models obtained from [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR) after conversion.

You can use our pre-converted files: [MaaCommonAssets](https://github.com/MaaXYZ/MaaCommonAssets/tree/main/OCR). Choose the language you need and store them according to the directory structure mentioned above in [Prepare Resource Files](#prepare-resource-files).

If needed, you can also fine-tune the official pre-trained models of PaddleOCR yourself (please refer to the official PaddleOCR documentation) and convert them to ONNX files for use. You can find conversion commands [here](https://github.com/MaaXYZ/MaaCommonAssets/tree/main/OCR#command).

## Debug

There are currently two ways: [MaaDebugger](https://github.com/MaaXYZ/MaaDebugger) and [VSCode plugin](https://marketplace.visualstudio.com/items?itemName=nekosu.maa-support).

### MaaDebugger

- If you use MaaPiCli, the `config/maa_option.json` file will be generated in the same directory, including:

  - `logging`: Save the log and generate `debug/maa.log`. Default true.
  - `recording`: Save recording function, which will save all screenshots and operation data during operation. You can use `DbgController` for reproducible debugging. Default false.
  - `save_draw`: Saves the image recognition visualization results. All image recognition visualization results drawings during the run will be saved. Default false.
  - `show_hit_draw`: Displays the task hit pop-up window. Each time the recognition is successful, a pop-up window will display the recognition result. Default false.
  - `stdout_level`: The console displays the log level. The default is 2 (Error), which can be set to 0 to turn off all console logs, or to 7 to turn on all console logs.

- If you integrate it yourself, you can enable debugging options through the `Toolkit.init_option` / `MaaToolkitConfigInitOption` interface. The generated json file is the same as above.

### VSCode Plugin

Compared to MaaDebugger, which focuses on debugging, the current VSCode plugin also has many functions such as screenshots and code completion. The specific content can be explored through the [official website](https://marketplace.visualstudio.com/items?itemName=nekosu.maa-support) and actual experience.

## Run

You can integrate MaaFramework using MaaPiCli (Generic CLI) or by writing integration code yourself.

### Using MaaPiCli

*‚≠êIf you use the project template, follow its documentation directly and run `install.py` to automatically package the relevant files.*

Use MaaPiCli in the `bin` folder of the Release package, and write `interface.json` and place it in the same directory to use it.

The Cli has completed basic function development, and more functions are being continuously improved! Detailed documentation needs to be further improved. Currently, you can refer to [Sample](https://github.com/MaaXYZ/MaaFramework/blob/main/sample/interface.json) to write it.

Examples:

- [M9A](https://github.com/MaaXYZ/M9A/tree/main/assets/interface.json)

### Writing Integration Code Yourself

Please refer to the [Integration Documentation](2.1-Integration.md).
